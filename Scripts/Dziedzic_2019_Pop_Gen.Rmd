---
title: "Pop_genomics_Meyer_2019"
author: "Matias Gomez"
date: "7/16/2019"
output: md_document
---
##Reanalysis of Dziedzic et al. (2019) O. faveolata data (https://www.ncbi.nlm.nih.gov//bioproject/PRJNA413258, https://www.ncbi.nlm.nih.gov/Traces/study/?query_key=2&WebEnv=NCID_1_14569396_130.14.18.48_5555_1574118114_1163998098_0MetA0_S_HStore&o=libraryselection_s%3Aa&s=SRR8844779,SRR8844780,SRR8844781,SRR8844782,SRR8844783,SRR8844784,SRR8844785,SRR8844786,SRR8844787,SRR8844788,SRR8844789,SRR8844790,SRR8844792,SRR8844793,SRR8844794,SRR8844795,SRR8844796,SRR8844797,SRR8844798,SRR8844799,SRR8844800,SRR8844801,SRR8844802,SRR8844803,SRR8844804,SRR8844805,SRR8844808,SRR8844809,SRR8844810,SRR8844812,SRR8844807,SRR8844806,SRR8844791,SRR8844776,SRR8844811,SRR8844778,SRR8844777,SRR8844775,SRR8844774)

##Activate previously created environment Orbicella
```{bash eval=FALSE }
conda activate Orbicella
```

## To see original pipeline by authors go to: http://eli-meyer.github.io/2bRAD_utilities/

##SNP filtering (http://www.ddocent.com/filtering/)
## First, let´s create a working dir and change to it, then copy the raw VCF file generetated by dDocent to start with the SNP filtering

```{bash eval=FALSE}
mkdir SNP_Filtering_Dziedzic_2019
cd SNP_Filtering_Dziedzic_2019
cp /RAID_STORAGE2/mgomez/Orbicella_Raw/Dziedzic_2019_Data/TotalRawSNPs.vcf .
```

##Adapted from http://eli-meyer.github.io/2bRAD_utilities/#filter and using part of dDdocent pipeline

## Typically we are only interested in polymorphic loci (SNPs). We apply this filter first, since most loci are monomorphic and excluding them will greatly reduce file sizes. This is accomplished by running the following script: PolyFilter.pl -i combined.tab -n 2 -p y -o snps.tab .In this example, we selected all loci at which 2 or more genotypes were observed, writing them to a file called "snps.tab". 

## I am going to keep only biallelic SNPs first

```{bash eval=FALSE}
vcftools --vcf TotalRawSNPs.vcf --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic

After filtering, kept 39 out of 39 Individuals
Outputting VCF file...
After filtering, kept 370231 out of a possible 383160 Sites
Run Time = 48.00 seconds
```

## As far as I understand, I need to keep loci with at least 2 o or more genotypes. I can also keep loci only genotyped in 90% of individuals

## we are going to use the program VCFtools (http://vcftools.sourceforge.net) to filter our vcf file. To make this file more manageable, let’s start by applying three step filter. We are going to only keep variants that have been successfully genotyped in 90% of individuals, a minimum quality score of 30, and a minor allele count of 3
## In this code, we call vcftools, feed it a vcf file after the --vcf flag, --max-missing 0.9 tells it to filter genotypes called below 90% (across all individuals) the --mac 2 flag tells it to filter SNPs that have a minor allele count less than 3. This is relative to genotypes, so it has to be called in at least 1 homozygote and 1 heterozygote or 3 heterozygotes. The --recode flag tells the program to write a new vcf file with the filters, --recode-INFO-all keeps all the INFO flags from the old vcf file in the new one. Lastly, --out designates the name of the output 

```{bash eval=FALSE}
vcftools --gzvcf TotalRawSNPs_BiAllelic.recode.vcf --max-missing 0.9 --mac 3 --minQ 30 --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic_mac3_miss90

After filtering, kept 39 out of 39 Individuals
Outputting VCF file...
After filtering, kept 7037 out of a possible 370231 Sites
Run Time = 4.00 seconds
```

##Now keep loci with a Minor Allele Freq of 2.6 or present in at least 1 individual (1*100/39= 2.57) and a mininmum depth of 5x

```{bash eval=FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90.recode.vcf  --maf 0.026 --min-meanDP 20 --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20 After filtering, kept 39 out of 39 Individuals
Outputting VCF file...
After filtering, kept 6007 out of a possible 7037 Sites
Run Time = 1.00 seconds 
```

##The next step is to get rid of individuals that did not sequence well. We can do this by assessing individual levels of missing data.

```{bash eval=FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf --missing-indv

```

##This will create an output called out.imiss. Let’s examine it.
```{bash }
cat out.imiss
```

##We can see that one  have as high as 30% missing data. Let’s take a look at a histogram

```{bash}
mawk '!/IN/' out.imiss | cut -f5 > totalmissing
gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin($1,binwidth)):(1.0) smooth freq with boxes
pause -1
EOF
```

##There is only one individuals with 30 % missing data. Now we need to create a list of individuals with more than 30 missing data. 
```{bash eval=FALSE}
mawk '$5 > 0.30581' out.imiss | cut -f1 > lowDP.indv
cat lowDP.indv
```

## Since it is only one individual with 30% missing information I decided to keep it for the time being.

##First, we need to create file that has two tab separated columns. First with the sample name, second with population assignment.

```{bash eval=FALSE}
vcf-query -l TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf > TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf.indv
cut -d "_" -f1 TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf.indv > TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf.indv.pop
paste TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf.indv TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf.indv.pop > popmap.txt
```

##You need to add headers the table before you uploaded to R: edit the table in R by clicking on the table and addding the headers separated by tab

```{r}
SNP_6007.pop <-read.table("/RAID_STORAGE2/mgomez/Orbicella_Raw/Dziedzic_2019_Data/SNP_Filtering_Dziedzic_2019/popmap.txt", header=TRUE)
SNP_6007.pop
```

# Convert the VCF file to BayeScan format using pgdspider

```{bash eval=FALSE}
java -jar /usr/local/bin/PGDSpider2-cli.jar -inputfile TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf  -outputfile TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan -spid BSsnp.spid
```

##Now scan for outlier loci with BayeScan (in case Bayescan does not run, try loading the vcf file to R and write a hierfstat file an then a bayescan one)

```{bash eval=FALSE}
BayeScan2.1_linux64bits TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan -o TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan.Output -n 5000 -thin 10 -nbp 20    -pilot 5000   -burn 50000 -pr_odds 100 -threads 20
```

```{bash eval=FALSE}
cp /RAID_STORAGE2/mgomez/Orbicella_Raw/Libraries_compiled/SNP_Filtering/plot_R.r .
```

```{r}
source("plot_R.r")
BayeScan_Results<-plot_bayescan("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan.Output_fst.txt",FDR=0.05)
BayeScan_Results$outliers
BayeScan_Results$nb_outliers

```

```{r}
BayeScan_Results_sel=read.table("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan.Output.sel",colClasses="numeric")
parameter="Fst1"
plot(density(BayeScan_Results_sel[[parameter]]),xlab=parameter,main=paste(parameter,"posterior distribution"))

parameter="logL"
plot(density(BayeScan_Results_sel[[parameter]]),xlab=parameter,main=paste(parameter,"posterior distribution"))

BayeScan_Results_Fst=read.table("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.BayesScan.Output_fst.txt",colClasses="numeric")
parameter="alpha"

plot(density(BayeScan_Results_Fst[[parameter]]),xlab=parameter,main=paste(parameter,"posterior distribution"))
```

## if you have the package "boa" installed, you can very easily obtain Highest Probability 
##Density Interval (HPDI) for your parameter of interest (example for the 95% interval):
## > boa.hpd(mydata[[parameter]],0.05)

```{r}
#install.packages("boa")
library(boa)
parameter="Fst1"
boa.hpd(BayeScan_Results_sel[[parameter]],0.05)
```

##Remove Indels and leave SNPs only 

```{bash eval = FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20.recode.vcf --remove-indels --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels


After filtering, kept 39 out of 39 Individuals
Outputting VCF file...
After filtering, kept 5354 out of a possible 6007 Sites
```

## We now need to keep SNPs that are not in LD, we want to keep SNPs that are at least 1000 pb apart 

```{bash eval=FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels.recode.vcf --thin 1000 --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000 

Outputting VCF file...
After filtering, kept 3560 out of a possible 5354 Sites
```

## Make a pop file for the next steps 

```{bash eval=FALSE}
vcf-query -l TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf > TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.indv
cut -d "_" -f1 TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.indv > TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.indv.pop
paste TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.indv TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.indv.pop > SNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_pop.txt
```

##You need to add headers the table before you uploaded to R: edit the table in R by clicking on the table and addding the headers separated by tab

```{r}
SNP_3560.pop <-read.table("/RAID_STORAGE2/mgomez/Orbicella_Raw/Dziedzic_2019_Data/SNP_Filtering_Dziedzic_2019/SNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_pop.txt", header=TRUE)
SNP_3560.pop 
```

## Edit file so pop is a number and not a string. This needs to be done in order to run structure since it will not take string in the population column. 
## I added a the column Pop_No and assigned each individual a 1

```{r}
SNP_3560.pop <-read.table("/RAID_STORAGE2/mgomez/Orbicella_Raw/Dziedzic_2019_Data/SNP_Filtering_Dziedzic_2019/SNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_pop.txt", header=TRUE)
SNP_3560.pop 
```

##Load VCF on R and convert it to Structure format

```{r eval=FALSE}
library(vcfR)
vcf_3560_SNPs <- read.vcfR("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf")
vcf_3560_SNPs 
```

## Now that we have a VCF thorougly filtered with unlinked, without indels and no outlier loci we can run Bayesian clustering as implemented in STRUCTURE software

##Running structure

##https://radcamp.github.io/NYC2019/05_STRUCTURE_API.html

##Although there are many newer and faster implementations of STRUCTURE, such as faststructure or admixture, the original STRUCTURE works much better with missing data, which is of course a common feature of RAD-seq data sets.

##We convert a VCF file to the structure format with the following steps. In this way a make the needed file with the pop labels but without the individuals labels. Even though this file does not have the individual`s labels, these are optinal for Structure so we can try running structure with this file to seee how it goes. 

##We start by loading the following R packages

```{r eval=FALSE}
library(adegenet)
library(hierfstat)
SNP_3560.genind <- vcfR2genind(vcf_3560_SNPs)
pop(SNP_3560.genind) <- SNP_3560.pop$Pop_No
indNames(SNP_3560.genind)<-SNP_3833.pop$Sample
indNames(SNP_3560.genind)
SNP_3560_ID<-SNP_3560.pop$Sample
SNP_3560.genind
SNP_3560.hierfstat <- genind2hierfstat(SNP_3560.genind)
SNP_3560.hierfstat

write.struct(SNP_3560.hierfstat, fname = "SNP_3560_STRUCTURE.str") 
```

##Automation and Parallelization of STRUCTURE Analysis (StrAuto)

##1. Make sure the following are installed on your computer/server:
##Python 2.7.x or 3.0.x (see folder labeled py3 for that version) 
##STRUCTURE 2.3.4 backend version (Pritchard et al., 2000)
#structureHARVESTER (Earl and vonHoldt, 2012) (Optional) 
#GNU Parallel (Tange, 2011) (Optional)

```{bash eval=FALSE}
which python 
which structure
conda install -c bioconda structure
wget https://www.crypticlineage.net/assets/strauto_1.tar.gz

conda install -c conda-forge parallel
```

##https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html
##Python 2.7.x or 3.0.x 

## Installing a different version of Python

##To install a different version of Python without overwriting the current version, create a new environment and install the second Python version into it:

##Create the new environment:

##To create the new environment for Python 3.6, in your terminal window or an Anaconda Prompt, run:
        
##If your previous environment is activated (Orbicella in my case), deactivate it (conda deactivate ) 

```{bash eval=FALSE}
conda create -n py27 python=2.7 anaconda
conda activate py27
conda install -c bioconda structureharvester
wget https://www.crypticlineage.net/assets/strauto_1.tar.gz
tar xvzf strauto_1.tar.gz
```

##I am going to try running the example data file
## We highly recommend that you set the burn-in and MCMC to 100 each and set up an example run with this data file. Once you verify that the trial run worked as intended, proceed with your own data. Also change cores to 20

```{bash eval=FALSE}
./strauto_1.py
```

##The program will present an introductory screen and wait for you to proceed. A carriage return allows the program to read the template input.py file. If the reading is successful, all parameter values will be displayed on screen for verification. If you find any errors, type (q)uit to exit the program and make corrections to your input file before proceeding. Once verified, type (a)ccept to begin writing the output files.

## A successful run of StrAuto will generate following files:

##$ ls -lh
   -rw-r--r--@  1 user  group   1.0K Apr  2 11:42 extraparams
   -rw-r--r--@  1 user  group   2.8K Apr  2 08:59 input.py
   -rw-r--r--@  1 user  group   567B Apr  2 11:42 mainparams
   -rwxr-xr-x@  1 user  group   634B Apr  2 11:42 runstructure
   -rw-r--r--@  1 user  group   6.6K Apr  2 11:42 structureCommands
   
   
## Now you have everything you need to begin your structure run. If you ran StrAuto on your local workstation, but want to perform the analysis on a remote server, copy this folder in its entirety to that server. Then proceed with the analysis as follows.

```{bash  eval = FALSE}
screen -S my_session -d -m ./runstructure
```

## Since the previous run was successful, I can now run my own data.
## First we modify the file input.py according to my own data.
## We use  k = 1-8, 
##Analyses were performed with five runs of 500,000 iterations each (250,000 burn‐in), with correlated allele frequencies and under the admixture model (Falush, Stephens, & Pritchard, 2003). (Kornillos et al, 2019)

## 1. How many populations are you assuming? [Integers]
maxpops = 8

## 2. How many burnin you wish to do before collecting data [Integers]
burnin = 250000

## 3. How long do you wish to collect the data after burnin [Integers]
mcmc = 500000

## 4. Name of your dataset.  Don't remove quotes. No spaces allowed. Exclude the '.str' extension.  
##    e.g. dataset = "sim" if your datafile is called 'sim.str'
dataset = "SNP_3560_STRUCTURE"

## 5. How many runs of Structure do you wish to do for every assumed cluster K? [Integers]
kruns = 10

## 6. Number of individuals [Integers]
numind = 39

## 7. Number of loci [Integers]
numloci = 3560

## 8. What is the ploidy [Integers 1 through n]
ploidy = 2

## 9. How is the missing data coded? Write inside quotes. e.g. missing = "-9"
missing = "-9"

## 10. Does the data file contain every individual on 2 lines (0) or 1 line (1). [Boolean]
onerowperind = 0 

## 11. Do the individuals have labels in the data file?  [Boolean]
label = False

## 12. Are populations identified in the data file? [Boolean]
popdata =  True

## 13. Do you wish to set the popflag parameter? [Boolean]
popflag = False

## 14. Does the data file contain location identifiers (Not the same as population identifier) [Boolean]
locdata = False

## 15. Does the data file contain phenotypic information? [Boolean]
pheno = False

## 16. Does the data file contain any extra columns before the genotype data begins? [Boolean]
extracols = False

## 17. Does the data file contain a row of marker names at the top? [Boolean]
markers = True

## 18. Are you using dominant markers such as AFLP? [Boolean]
dominant = False

## 19. Does the data file contain information on map locations for individual markers? [Boolean]
mapdist = False

## 20. Is the data in correct phase? [Boolean]
phase = True

## 21. Is the phase information provided in the data? [Boolean]
phaseinfo = False

## 22. Does the phase follow markov chain? [Boolean]
markov = False

## 23. Do you want to make use of parallel processing [Boolean]
##     Note that you must have GNU parallel installed and available
##     www.gnu.org/s/parallel
##     You can check availability of the program by running 'which parallel' at the 
##     command prompt. If a destination of the file is returned, then it is available.
##     If not available, it must be installed locally or through your system administrator.

parallel = True

## 24. How would you like to define the number of cores for parallel processing ['number' or 'percent']
##     Use 'percent' if you would like to define the percentage of available cores to use.  For instance,
##     on a quad-core machine you might use 'percent' here and '75' for cores to use 3 of the 4 processors.
##     Use 'number' if you want to explicitely define the number of cores used.  

core_def = 'number'

## 25. How many cores would you like to use [integer]
##     This represents either a pecentage or the physical number of cores as defined in core_def (#24).

cores = 20

## 26. Would you like to automatically run structureHarvester?  [boolean]
##     Note that you must have program installed and available.
##     https://users.soe.ucsc.edu/~dearl/software/structureHarvester/

harvest = True

########## End of questions ##########
########## Please do not write any other information in this file ###########

```{bash eval = FALSE}
./strauto_1.py
```

```{ bash eval = FALSE}
screen -S my_session -d -m ./runstructure
```

## Once you have the results from Automation and Parallelization of STRUCTURE Analysis (StrAuto), go to http://taylor0.biology.ucla.edu/structureHarvester/ and upload the file ./harvester/SNP_3833_STRUCTURE_Harvester_Upload.zip to this server. Download the files results from the server and (remane the file) upload them to a directory called Harvester_results working directory and unzip it.

```{bash eval=FALSE}

tar xvzf Harvester_results.gz 
```

## I haven´t done from the command line yet. I did it from the web sever (http://clumpak.tau.ac.il/) (http://clumpak.tau.ac.il/results.html?jobId=1566440331) and uploaded the results

##Download the distruct software from https://web.stanford.edu/group/rosenberglab/distruct.html after registration and unzip it 

```{bash eval=FALSE}
wget https://web.stanford.edu/group/rosenberglab/software/distruct1.1.tar.gz
tar xvzf distruct1.1.tar.gz

```


## Admixture 

## Converting VCF files to PLINK format

##(http://evomics.org/learning/population-and-speciation-genomics/2016-population-and-speciation-genomics/fileformats-vcftools-plink/#ex2.3)

```{bash eval=FALSE}
bcftools view -H TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf | cut -f 1 | uniq | awk ‘{print $0″\t”$0}’ > TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.chrom-map.txt
```





## VCFtools can convert VCF files into formats convenient for use in other programs. One such example is the ability to convert into PLINK format. The following function will output the variants in .ped and .map files.

```{bash eval=false}
  vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf --plink --chr 1 --out TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_Plink
```





##Most code, if not all, taken from: https://grunwaldlab.github.io/Population_Genetics_in_R/gbs_analysis.html

##PCA 

##A principal components analysis (PCA) converts the observed SNP data into a set of values of linearly
##uncorrelated variables called principal components that summarize the variation between samples.
##We can perform a PCA on our genlight object by using the glPCA function.
#To view the results of the PCA we can use the package ggplot2. We need to convert the data frame
#that contains the principal components (Orbicella_genlight_pca$scores) into the new object Orbicella_genlight_pca_scores. In addition, we will add the population values as a new column in our Orbicella_genlight_pca_scores object, in order to be able to color samples by population.

#ggplot2 will plot the PCA, color the samples by population, and create ellipses that include 95% of the data for each the population:

```{r}
library(ggplot2)
library(RColorBrewer)
library(colorspace)
#Install below package if necessary
#install.packages("poppr")
library(poppr)
library(colorspace)
library(vcfR)



VCF_3560_SNPs<-read.vcfR("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf")
VCF_3560_SNPs

genlight_3560_SNPs <- vcfR2genlight(VCF_3560_SNPs)
genlight_3560_SNPs


genlight_3560_SNPs_pca <- glPca(genlight_3560_SNPs, nf=3)

genlight_3560_SNPs_pca_scores <- as.data.frame(genlight_3560_SNPs_pca$scores)
genlight_3560_SNPs_pca_scores


set.seed(9)
p <- ggplot(genlight_3560_SNPs_pca_scores, aes(x=PC1, y=PC2))
p <- p + geom_point(size=2)
p <- p + stat_ellipse(level = 0.95, size = 1)
p <- p + geom_hline(yintercept = 0)
p <- p + geom_vline(xintercept = 0)
theme<-theme(panel.background = element_blank(),panel.border=element_rect(fill=NA),panel.grid.major =
               element_blank(),panel.grid.minor = element_blank(),strip.background=element_blank(),
             axis.text.x=element_text(colour="black"),axis.text.y=element_text(colour="black"),
             axis.ticks=element_line(colour="black"),plot.margin=unit(c(1,1,1,1),"line"))
p <- p +theme
p
```

```{r}
barplot(100*genlight_3560_SNPs_pca$eig/sum(genlight_3560_SNPs_pca$eig), col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Percent of variance\nexplained", line = 2)
title(xlab="Eigenvalues", line = 1)
head (genlight_3560_SNPs_pca$eig)
```
##DAPC
##We can further explore population assignments using a discriminant analysis of principal components (DAPC).
## Since we do not have any previous information about genetic structure, and all colonies come from the same geographical population, we need to find natural clusters in our data. We first use find.cluster (cluster identification using successive K-means): These functions implement the clustering procedure used in Discriminant Analysis of Principal Components (DAPC, Jombart et al. 2010). This procedure consists in running successive K-means with an increasing number of clusters (k), after transforming data using a principal component anal- ysis (PCA). For each model, a statistical measure of goodness of fit (by default, BIC) is computed, which allows to choose the optimal k.

```{r}
clusters_genlight_3560_SNPs <- find.clusters(genlight_3560_SNPs, max.n.clust = 10, n.pca =39, n.clust = 4  )
clusters_genlight_3560_SNPs
clusters_genlight_3560_SNPs$grp
#Choose the number PCs to retain (>=1): 
#39
#Choose the number of clusters (>=2: 
#4

```

## We choose to retain 13 PC that explained 80 % of the observed variance. If too few PCs (with respect to the number of individuals) are retained, useful information will be excluded from the analysis, and the resultant model will not be informative enough to accurately discriminate between groups. By contrast, if too many PCs are retained, this will have a destabilising effect on the coefficients extimated, leading to problems of overfit (adegenet package)

## Based on the lowest BIC value we choose  clusters (k=4)

```{r}
pop(genlight_3560_SNPs)<- clusters_genlight_3560_SNPs$grp
dapc_3560_SNPs <- dapc(genlight_3560_SNPs, n.pca = 39, n.da = 4)
dapc_3560_SNPs
scatter.dapc(dapc_3560_SNPs, scree.pca = TRUE)
```


## This DAPC scatter plot shows 4 grpups because qe overstimated by chossing all the PC. This is way we know want to evaluate the optimal number of PC to retain through the alfa score

## DAPC requires enough PCs to secure a space with sufficient power of discrimination but must also avoid retaining too many dimensions that lead to over-fitting.
## Using the a-score to measure the trade-off between power of discrimination and over-fitting. This score is simply the difference between the proportion of successful reassignment of the analysis (observed discrimination) and values obtained using random groups (random discrimination). It can be seen as the proportion of successful reassignment corrected for the number of retained PCs. It is implemented by a.score, which relies on repeating the DAPC analysis using randomized groups, and computing a-scores for each group, as well as the average a-score: 

```{r}
a_score <- optim.a.score(dapc_3560_SNPs)  
a_score

```

## Now we run dapc again but knowing that 6 PC is the optimal number and we keep the cluster assignation (k=4) previouly found with find.clusters. And wee keep 2 discriminant functions

```{r}
library(RColorBrewer)
pop(genlight_3560_SNPs) <- clusters_genlight_3560_SNPs$grp
colors <- brewer.pal(n = nPop(genlight_3560_SNPs), name = "Dark2")
dapc_6_PC <- dapc(genlight_3560_SNPs,clusters_genlight_3560_SNPs$grp, n.pca = 6, n.da = 2)
dapc_6_PC
dapc_6_PC$var
dapc_6_PC$grp

scatter(dapc_6_PC, col = colors, scree.pca = TRUE, scree.da = TRUE, posi.da = "topleft", posi.pca = "bottomleft")
```


## Improving the DAPC chart: make the fond bigger and define the color palette 

```{r eval=FALSE}
install.packages("palettetown")
library(palettetown)

```
 
```{r}
colors <- (values=c("#E84838","#5088B8","#78b850","#385860"))
scatter(dapc_6_PC,scree.da = FALSE, col =colors, cex=4, cstar=0, solid=.4,clab=0, leg=TRUE, txt.leg=paste("Cluster",1:4), posi.leg = "topleft" )
clusters_genlight_3560_SNPs$grp
par(xpd=TRUE)

```
 
```{r}
dapc_6_PC_1da <- dapc(genlight_3560_SNPs,clusters_genlight_3560_SNPs$grp, n.pca = 6, n.da = 1)
dapc_6_PC_1da
	
scatter (dapc_6_PC_1da, col = colors, posi.da = "bottomright", cex=4)
```


## Structure-like plot from DAPC 

```{r eval}
compoplot(dapc_6_PC, col =colors)
```


```{r}
maxK <- 8
myMat <- matrix(nrow=8, ncol=maxK)
colnames(myMat) <- 1:ncol(myMat)
for(i in 1:nrow(myMat)){
  grp <- find.clusters(genlight_3560_SNPs, n.pca = 6, choose.n.clust = FALSE,  max.n.clust = maxK)
  myMat[i,] <- grp$Kstat
}

library(ggplot2)
library(reshape2)
my_df <- melt(myMat)
colnames(my_df)[1:3] <- c("Group", "K", "BIC")
my_df$K <- as.factor(my_df$K)
head(my_df)

p1 <- ggplot(my_df, aes(x = K, y = BIC))
p1 <- p1 + geom_boxplot()
p1 <- p1 + theme_bw()
p1 <- p1 + xlab("Number of groups (K)")
p1

my_k <- 2:8

grp_l <- vector(mode = "list", length = length(my_k))
dapc_l <- vector(mode = "list", length = length(my_k))

for(i in 1:length(dapc_l)){
  set.seed(9)
  grp_l[[i]] <- find.clusters(genlight_3560_SNPs, n.pca = 6, n.clust = my_k[i])
  dapc_l[[i]] <- dapc(genlight_3560_SNPs, pop = grp_l[[i]]$grp, n.pca = 6, n.da = my_k[i])}

my_df <- as.data.frame(dapc_l[[ length(dapc_l) ]]$ind.coord)
my_df$Group <- dapc_l[[ length(dapc_l) ]]$grp
head(my_df)

my_pal <- RColorBrewer::brewer.pal(n=8, name = "Dark2")

p2 <- ggplot(my_df, aes(x = LD1, y = LD2, color = Group, fill = Group))
p2 <- p2 + geom_point(size = 4, shape = 21)
p2 <- p2 + theme_bw()
p2 <- p2 + scale_color_manual(values=c(my_pal))
p2 <- p2 + scale_fill_manual(values=c(paste(my_pal, "66", sep = "")))
p2

tmp <- as.data.frame(dapc_l[[1]]$posterior)
tmp$K <- my_k[1]
tmp$Isolate <- rownames(tmp)
tmp <- melt(tmp, id = c("Isolate", "K"))
names(tmp)[3:4] <- c("Group", "Posterior")
tmp$Region <- genlight_3560_SNPs$pop
my_df <- tmp

for(i in 2:length(dapc_l)){
  tmp <- as.data.frame(dapc_l[[i]]$posterior)
  tmp$K <- my_k[i]
  tmp$Isolate <- rownames(tmp)
  tmp <- melt(tmp, id = c("Isolate", "K"))
  names(tmp)[3:4] <- c("Group", "Posterior")
  tmp$Region <- genlight_3560_SNPs$pop

  my_df <- rbind(my_df, tmp)
}

grp.labs <- paste("K =", my_k)
names(grp.labs) <- my_k

p3 <- ggplot(my_df, aes(x = Isolate, y = Posterior, fill = Group))
p3 <- p3 + geom_bar(stat = "identity")
p3 <- p3 + facet_grid(K ~ Region, scales = "free_x", space = "free", 
                      labeller = labeller(K = grp.labs))
p3 <- p3 + theme_bw()
p3 <- p3 + ylab("Posterior membership probability")
p3 <- p3 + theme(legend.position='none')
#p3 <- p3 + scale_color_brewer(palette="Dark2")
p3 <- p3 + scale_fill_manual(values=c(my_pal))
p3 <- p3 + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10))
p3

# http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/
library("ggpubr")
#tiff('dapc__k3_5_dapc.tiff', width=6.5, height=6.5, units='in', compression='lzw', res=300)
ggarrange(ggarrange(p1,
                    p2,
                    ncol = 2, labels = c("A", "B")),
          p3,
          nrow = 2,
          labels = c("", "C"),
          heights = c(1, 2)
          )
```


## Now we run the same DAPC analysis but keeping one discriminant function to see visualise it

```{r}
dapc_6_PC_1da <- dapc(genlight_3560_SNPs,clusters_genlight_3560_SNPs$grp, n.pca = 6, n.da = 1)
dapc_6_PC_1da
	
scatter (dapc_6_PC_1da, col = colors, posi.da = "bottomright", posi.pca = "bottomleft")

```

##Distance matrices
##Let’s create a pairwise genetic distance matrix for individuals or populations (i.e., groups of individuals).
##To summarize, we can create a distance matrix from a genlight object using dist():

```{r}
Orbicella_tree <- aboot(genlight_3560_SNPs, tree = "upgma", distance = bitwise.dist, sample = 100, showtree = T, cutoff = 50, quiet = T)
```

```{r}
library(poppr)
library(ape)

pop(genlight_3560_SNPs)
pop(genlight_3560_SNPs)<- dapc_6_PC$grp
colors <- brewer.pal(n = nPop(genlight_3560_SNPs), name = "Dark2")
plot.phylo(Orbicella_tree, cex = 0.8, font = 2, adj = 0, tip.color =  colors[pop(genlight_3560_SNPs)])
nodelabels(Orbicella_tree$node.label, adj = c(1.3, -0.5), frame = "n", cex = 0.8,font = 3, xpd = TRUE)
legend('topleft', legend = c("1","2","3", "4"), fill = colors, border = FALSE, bty = "o", cex = 1)
axis(side = 1)
title(xlab = "Genetic distance (proportion of loci that are different)")
```

## Subsetting a vcfR object to 200 random variants

```{r}
subset_1 <- sample(size = 200, x= c(1:nrow(VCF_3560_SNPs))) 
subset_2 <- sample(size = 200, x= c(1:nrow(VCF_3560_SNPs)))
```

## Making sure no subsamples are identical
```{r}
my_vcf_sub1 <- VCF_3560_SNPs[subset_1,]
my_vcf_sub2 <- VCF_3560_SNPs[subset_2,]

my_vcf_sub1
my_vcf_sub2 

identical(my_vcf_sub1, my_vcf_sub2)
```

## Creating 50 subset vcf files
```{r}
# Creating a list object to save our subsets in.
my_vfc_variant_subset <- vector(mode = "list", length = 50)

# Using a for loop to generate 50 subsets of 200 random variants from the rubi.VCF vcfR object.
for (i in 1:50){
  my_vfc_variant_subset[[i]] <- VCF_3560_SNPs[sample(size = 200, x= c(1:nrow(VCF_3560_SNPs)))]
}

# Checking we have 50 vcfR objects:
length(my_vfc_variant_subset)
```

```{r}
head(my_vfc_variant_subset, n=3)
```

## Making and overlapping multile trees
```{r}
# Creating the GenLight object
my_vcf_gl_subset <- lapply(my_vfc_variant_subset, function (x) suppressWarnings(vcfR2genlight(x)))
for (i in 1:length(my_vcf_gl_subset)){
  ploidy(my_vcf_gl_subset[[i]]) <- 2
}

# Creating a simple UPGMA tree per object
library(phangorn)
my_vcf_trees <- lapply(my_vcf_gl_subset, function (x) upgma(bitwise.dist(x)))
class(my_vcf_trees) <- "multiPhylo"

# Overlapping the trees
densiTree(my_vcf_trees, consensus = Orbicella_tree, scaleX = T, show.tip.label = F, alpha = 0.1)
title(xlab = "Proportion of variants different")
```


## Colonies not bleached: 38, 41, 15, 8, 37, 16, 36, 20 and 10.
## Clades: 38 (2), 41(2), 15(), 8(), 37(), 16(), 36(), 20() and 10().

## SNAPP with Beast2: install Beast2 (https://www.beast2.org/beast-v2-0-readme-file/)

```{ bash eval = FALSE}
conda install -c bioconda beast2
```

##Install packages needed to run SNAPP in Beast2 
## https://www.beast2.org/managing-packages/

## For computers without GUI, like high performance clusters, packages can be managed through the command line. An application called ‘packagemanager’ is part of the Linux distribution of BEAST, and has the following options: 

##Usage: packagemanager [-list] [-add <NAME>] [-del <NAME>] [-useAppDir] [-dir <DIR>] [-help] 
   -list List available packages
   -add Install the <NAME> package 
   -del Uninstall the <NAME> package 
   -useAppDir Use application (system wide) installation directory. Note this requires writing rights to the application directory. If not specified, the user's BEAST directory will be used.
   -dir Install/uninstall package in direcotry <DIR>. This overrides the useAppDir option
   -help Show help

```{bash  eval = FALSE}
packagemanager -list
packagemanager -add SNAPP
packagemanager -add  MODEL_SELECTION
```

## We need our SNPS data in a NEXUS format
## Convert vcf to Nexus with PGDSpider standalone first. The nexus format must be in a binary one.

## I need to pick four samples per group based on the clusters formed. I also need to consider samples without missing info

## Explore by plotting the missing percentage of info  in the vcf 

```{bash eval=FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels.recode.vcf --missing-indv

```

```{bash eval=FALSE}
cat out.imiss

```

## From the above table we need to get rid of at least the samples that have at least 1% of missing info and keep 5 individuals 

###Preliminary runs
##After having converted the VCF file to nexus (binary) in PGDSpider, I edited the file on beauti following the Bayes Factor Delimitation tutorial (available on the beast website).

## 6.4 Running the XML file with BEAST (Species Trees and Species Delimitation with SNAPP: A Tutorial and Worked Example)
##Step 12: You can execute the XML file in BEAST using the GUI or the command line. If you are using Mac OSX or Windows, you should be able to launch the BEAST GUI by double clicking on the application icon. After the BEAST window appears, click the Choose File... button, and select the XML file you just created (Figure 12). Increase the Thread pool size to speed up your analysis. Running SNAPP with multiple threads can increase speeds, but experimenting with the number of threads is required to get the best performance.
##Click Run. The analysis should take about 10 minutes. You can also run BEAST from the command line. Open the Terminal Application and navigate to the folder containing your runA.xml file. To execute the file, type the following at the command line:
##/path/to/BEASTv2.4.1/bin/beast -threads 8 runA.xml or beast -threads 8 runA.xml. If you have already moved the BEAST executable to your path. Caution: setting the number of threads beyond the maximum number available on your computer can have serious drawbacks, and you will probably not have enough memory to support all of those separate analyses.

## Make a working directory and a subfolder for each model. This RunA folder is for subsample of 12 samples and 2 species

```{bash eval=FALSE}
mkdir SNAPP/RunA
cd SNAPP/RunA
```

##Now run beast 

##RunA: 2 species
```{bash eval=FALSE}
beast -threads 20 SNAPP/RunA/RunA.xml
marginal L estimate = -62648.156437294034
```

##6.5 Inspecting path sampling results
Step 13: At the end of your analysis, the path sampling results will be displayed on the screen. An example is shown in Figure 13. Each row shows the results from one path sampling step. The example in Figure 13 shows the results from a path sampling analysis with 24 steps. You will use the value after "marginal L estimate" to compare models.

## 6.6 Setting up new species delimitation models
Step 14: Now that you have one XML file up and running it is easy to make new XML files for each species delimitation model. To prepare a new file for species delimitation, make a few slight modifications to the existing runA.xml file: 
(1) save a copy of the xml file as runB.xml and save it in a new folder with the same name, 

## This RunB:3 species 
```{bash eval=FALSE}
mkdir SNAPP/RunB
cp SNAPP/RunA/RunA.xml SNAPP/RunB/
mv SNAPP/RunB/RunA.xml SNAPP/RunB/RunB.xml
```


(2) change the file stem names in the xml file so that you don’t accidentally overwrite any of your previous results, 
(3) edit the path sampling root directory to point to the new runB folder,
4) change the species assignments listed in the “stateDistribution” element. This last part requires changing the number and/or composition of taxonset features. Each taxonset begins with “<taxonset ...>” and ends with “</taxonset>” (Figure 14). To lump species, simply combine the taxon names into a single taxonset feature. To split a species, simple create a new taxonset containing the appropriate taxon names. To reassign a taxon to another species you can cut and paste the taxon to a different taxonset. XML files containing the species assignments shown in Figure 1 are provided with this tutorial (see Box 1). The XML files included with this tutorial contain a reduced number of samples in the taxonset blocks to help speed up the analyses. You do not have to exclude the samples from the data matrix and rerun BEAUTi. It is much more efficient to simply edit the taxonset features to include only those samples that you intend to anlayze.

```{bash eval=FALSE}
beast -threads 20 SNAPP/RunB/RunB.xml 
marginal L estimate = -61120.77565438789
```

## RunC: 4 species

```{bash eval=FALSE}
mkdir SNAPP/RunC
cp SNAPP/RunB/RunB.xml SNAPP/RunC/
mv SNAPP/RunC/RunB.xml SNAPP/RunC/RunC.xml

```

```{bash eval=FALSE}
beast -threads 20 SNAPP/RunC/RunC.xml 
marginal L estimate = -60474.428685611114
```

##I am now going to run Beast with the following parameters:
## MCMC length = 1,000,000, pre-burnin = 1000, samplefreq = 1000 (Quattrini et al., 2019 )

##First yo have modify every xml file to set the aforementioned parameters:

chainLength="1000000" 
storeEvery="1000"
preBurnin="1000" 
logEvery="1000"

##RunA: 2 species
```{bash eval=FALSE}
beast -threads 6 SNAPP/RunA/RunA.xml

```

## disable terminal input and output.

##To do this:
##Press control and Z simultaneously
##Type 'bg' without the quotes and press enter
##Type 'disown -h' again without the quotes and press enter

## This RunB:3 species 
```{bash eval=FALSE}
beast -threads 6 SNAPP/RunB/RunB.xml 

```

## disable terminal input and output.
##To do this:
##Press control and Z simultaneously
##Type 'bg' without the quotes and press enter
##Type 'disown -h' again without the quotes and press enter

## RunC: 4 species


```{bash eval=FALSE}
beast -threads 6 SNAPP/RunC/RunC.xml 

```

## disable terminal input and output.
##To do this:
##Press control and Z simultaneously
##Type 'bg' without the quotes and press enter
##Type 'disown -h' again without the quotes and press enter



##Step 15: After you run each of the alternative species delimitation models you can rank them by their marginal likelihood estimate (MLE). You can also calculate Bayes factors to compare the models. The Bayes factor (BF) is a model selection tool that is simple and well suited for the purposes of comparing species delimitation models. Calculating the BF between models is simple. To do so, simply subtract the MLE values for two models, and then multiply the difference by two (BF = 2 x (model1 - model2).
##The strength of support from BF comparisons of competing models can be evaluated using the framework of Kass and Raftery (1995). The BF scale is as follows: 0 < BF < 2 is not worth more than a bare mention, 2 < BF < 6 is positive evidence, 6 < BF < 10 is strong support, and BF > 10 is decisive.
##The results for the seven gecko models are provided in Table 2. The model that lumps the western forests into one species (runB) is the top-ranked model. It has the largest MLE value, and it is supported in favor of the current taxonomy model (runA). The BF in support for model B is decisive compared to model A. It is important to emphasize that these results are tragically deficient in terms of the MCMC analysis. Much, much longer runs are required to obtain stable results.

## RAxML-NG (https://github.com/amkozlov/raxml-ng/wiki/Tutorial)
## (https://github.com/amkozlov/raxml-ng/wiki/Installation)
```{bash eval=FALSE}
 conda install -c bioconda raxml-ng 
```


## Quartets in PAUP
## I was not able to install PAUP in the Cluster so I downloaded it and ran in my computer.
##First, using the PGDSPider I converted the vcf file into Fasta and then from Fasta to Nexus in PGDSpider as well. I used the the following files for the conversion: vcf_2_fasta.spid and Fasta_2_Nexus.spid
## I then had to edit the file manually to include the needed blocks of text to run the Quartets analysis and output the tree to edit in ITOL.

## Running PAUP in my computer i used the following code

```{bash eval=FALSE}
EXE PAN_Ofav_SVD_QUARTETS_tree.nex
```











##################################################################################################################

## SVD-Quartets in Tetrad (https://github.com/eaton-lab/tetrad)
## (https://github.com/dereneaton/ipyrad/blob/master/docs/tetrad.rst)

```{bash  eval = FALSE}
conda install tetrad -c eaton-lab -c conda-forge
conda install --channel=numba llvmlite
```


## Trying to convert VCF file to HDF5 format to run tetrad (https://scikit-allel.readthedocs.io/en/stable/index.html)
## Convert VCF to HDF5 and encode linkage block size.

```{bash  eval = FALSE}
conda install -c conda-forge scikit-allel
conda install -c anaconda python 
```

## https://ipyrad.readthedocs.io/en/latest/API-analysis/cookbook-vcf2hdf5.html
## You need to run in python by tipying "python". Once in python, type the following

import ipyrad.analysis as ipa
import pandas as pd

# init a conversion tool
converter = ipa.vcf_to_hdf5(
    name="TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000",
    data="/RAID_STORAGE2/mgomez/Orbicella_Raw/Dziedzic_2019_Data/SNP_Filtering_Dziedzic_2019/SVD_Quartets/TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf",
    ld_block_size=1000,
)

# run the converter
converter.run()











```{r eval=FALSE}
install.packages("tidyverse")
install.packages("Seurat")
remotes::install_github("UCSF-TI/fake-hdf5r")
library ("hdf5r")
```








-s seq            path to input phylip file (SNPs of full sequence file)
-m method         method for sampling quartets (all, random, or equal)
-q nquartets      number of quartets to sample (if not -m all)
  -b boots          number of non-parametric bootstrap replicates
  -n name           output name prefix (default: 'test')










```{bash eval=false}
tetrad -i TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf -o outdir -n TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000 
```



## SVD-Quartets in PAUP

## PAUP: http://paup.phylosolutions.com/tutorials/quick-start/
## Download PAUP (http://phylosolutions.com/paup-test/)
## Note: After gunzip'ing the downloaded file (if necessary), you may need to make it executable using a command like chmod a+x paup4a166_osx.

```{bash  eval = FALSE}
wget phylosolutions.com/paup-test/paup4a166_ubuntu64.gz
gunzip paup4a166_ubuntu64.gz
chmod a+x paup4a166_ubuntu64

wget phylosolutions.com/paup-test/paup4a166_centos64.gz
gunzip paup4a166_centos64.gz
chmod a+x paup4a166_centos64
chmod a+x paup4a166_centos64
```


chmod a+x -v paup4a166_centos64
chmod a+x -v paup4a166_ubuntu64


http://www.filepermissions.com/file-permission/0777



##Also, note that these command-line versions need the Fortran runtime library to be available. If the program does not start due to a missing library, try installing the GFortran compiler (or something that will install the needed library, such as R, LAPACK, or Octave). 

```{bash  eval = FALSE}
conda install -c conda-forge fortran-compiler
conda install -c conda-forge lapack
conda install -c conda-forge octave
```


##Make SVD_Quartets dir anc change to it

```{bash eval = FALSE}}
mkdir SVD_Quartets
cd SVD_Quartets
```

## I first need my last VCF (TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf) in nexus format.
## I will download this file and converted to nexus using pgdpsider GUI beacuse I have not beeen able to do it from the command line using PGDSpider
## The file is TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_NO_Ofav22.nex



##Trying to instal jupyter notebooks to run ipyrad pipeline

## Trying to run SVD-Quartets with the ipyrad pipeline 
## (https://nbviewer.jupyter.org/github/dereneaton/ipyrad/blob/master/tests/cookbook-tetrad.ipynb)

## The ipyrad.analysis package includes a module and command-line tool called tetrad that can be used to infer quartets from very large SNP alignments, and to join the quartets into a species tree for large numbers of samples following the methodology outlined by Chifman and Kubatko (2014) and implemented in SVDQuartets.


##This is a Jupyter notebook, a reproducible and executable document. The code in this notebook is Python (2.7), and should be executed in a jupyter-notebook like this one. Execute each cell in order to reproduce our entire analysis. We make use of the ipyparallel Python library to distribute STRUCTURE jobs across processers in parallel. If that is confusing, see our tutorial on using ipcluster with jupyter. The example data set used in this analysis is from the empirical example ipyrad tutorial.

## The code in this notebook is Python (2.7)

##https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html
##Python 2.7.x or 3.0.x 

## Installing a different version of Python

##To install a different version of Python without overwriting the current version, create a new environment and install the second Python version into it:

##Create the new environment:

##To create the new environment for Python 3.6, in your terminal window or an Anaconda Prompt, run:
        
##If your previous environment is activated (Orbicella in my case), deactivate it (conda deactivate ) 


## Since I had already created another conda environment for phyton 2.7 to run Structure, I only need to run "deactivate" to deactivate the current environment (Orbicella in my case)
the Orbicella environment and activate th previous environment
```{bash eval=FALSE}
conda deactivate Orbicella
conda activate py27
conda install jupyter
```



## Required software, All software required for this notebook can be installed using conda.

```{bash eval = FALSE}
conda install ipyrad -c ipyrad
conda install toytree -c eaton-lab
```


## https://github.com/rstudio/reticulate
## The reticulate package provides a comprehensive set of tools for interoperability between Python and R


```{r eval=FALSE}
install.packages("reticulate")
library(reticulate)
```


##Importing Python modules

You can use the import() function to import any Python module and call it from R

```{r}
use
ipyrad <- import(ipyrad) 

```



```{python}
import ipyrad as ip
import ipyrad.analysis as ipa
import ipyparallel as ipp
import toytree
```


```{phyton}
import ipyrad as ip
import ipyrad.analysis as ipa
import ipyparallel as ipp
import toytree
```



## PAUP: http://paup.phylosolutions.com/tutorials/quick-start/
## Download PAUP (http://phylosolutions.com/paup-test/)
## Note: After gunzip'ing the downloaded file (if necessary), you may need to make it executable using a command like chmod a+x paup4a166_osx.

```{bash  eval = FALSE}
wget phylosolutions.com/paup-test/paup4a166_osx.gz
gunzip paup4a166_osx.gz
chmod a+x paup4a166_osx
```

##Also, note that these command-line versions need the Fortran runtime library to be available. If the program does not start due to a missing library, try installing the GFortran compiler (or something that will install the needed library, such as R, LAPACK, or Octave). 

```{bash  eval = FALSE}
conda install -c conda-forge fortran-compiler
```

##################################################################################################################################################
## Do PCA, DAPC and Structure analyses taking out colony Ofav22 that has 30% of missing information

## First make a file with the sample to remove: PAN_Ofav22

```{bash eval =FALSE}
nano Delete_PAN_Ofav22
```

## Use vcf tools to remove the sample from the vcf file

```{bash eval =FALSE}
vcftools --vcf TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000.recode.vcf --remove Delete_PAN_Ofav22 --recode --recode-INFO-all --out TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_NO_Ofav22

After filtering, kept 38 out of 39 Individuals
Outputting VCF file...
After filtering, kept 3560 out of a possible 3560 Sites
Run Time = 3.00 seconds
```

##Now rerun PCA

```{r}
VCF_3560_SNPs_No_Ofav22<-read.vcfR("TotalRawSNPs_BiAllelic_mac3_miss90_maf0.026_dp20_NoIndels_LD1000_NO_Ofav22.recode.vcf")
VCF_3560_SNPs_No_Ofav22

genlight_3560_No_Ofav22<- vcfR2genlight(VCF_3560_SNPs_No_Ofav22)
genlight_3560_No_Ofav22


#genlight_3560_No_Ofav22_pca <- glPca(genlight_3560_No_Ofav22)
genlight_3560_No_Ofav22_pca <- glPca(genlight_3560_No_Ofav22, nf= 3)
genlight_3560_No_Ofav22_pca_scores <- as.data.frame(genlight_3560_No_Ofav22_pca$scores)
genlight_3560_No_Ofav22_pca_scores


set.seed(9)
p <- ggplot(genlight_3560_No_Ofav22_pca_scores, aes(x=PC1, y=PC2))
p <- p + geom_point(size=2)
p <- p + stat_ellipse(level = 0.95, size = 1)
p <- p + geom_hline(yintercept = 0)
p <- p + geom_vline(xintercept = 0)
theme<-theme(panel.background = element_blank(),panel.border=element_rect(fill=NA),panel.grid.major =
               element_blank(),panel.grid.minor = element_blank(),strip.background=element_blank(),
             axis.text.x=element_text(colour="black"),axis.text.y=element_text(colour="black"),
             axis.ticks=element_line(colour="black"),plot.margin=unit(c(1,1,1,1),"line"))
p <- p +theme
p
```

```{r}
barplot(100*genlight_3560_No_Ofav22_pca$eig/sum(genlight_3560_No_Ofav22_pca$eig), col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Percent of variance\nexplained", line = 2)
title(xlab="Eigenvalues", line = 1)
head (genlight_3560_No_Ofav22_pca$eig)
```
```{r}
clusters_genlight_3560_SNPs_No_Ofav22 <- find.clusters(genlight_3560_No_Ofav22, max.n.clust = 10, n.pca =39, n.clust = 4 )
clusters_genlight_3560_SNPs
clusters_genlight_3560_SNPs$grp
#Choose the number PCs to retain (>=1): 
#39
#Choose the number of clusters (>=2: 
#4

```

##Attempt to use find.clusters without choosing the number of clusters nad usding the criterion of "diffNgroup" and the otimal number of PC`s of 6. Dataset with all individuals

```{r}
clusters_genlight_3560_SNPs <- find.clusters(genlight_3560_SNPs, choose.n.clust = FALSE, criterion = "diffNgroup", n.pca = 6 )
clusters_genlight_3560_SNPs
```


Orbicella_clusters2 <- find.clusters(Orbicella_genlight_4085_SNP, choose.n.clust = FALSE, criterion = "diffNgroup", n.pca = 21)
